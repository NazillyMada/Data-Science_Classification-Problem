{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599802575584",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Heart\n",
    "\n",
    "Some classification problem on heart.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File ditemukan\n"
    }
   ],
   "source": [
    "directory = os.path.join('D:/Bootcamp ML - Mada/classification dataset/', 'heart.csv')\n",
    "if os.path.isfile(directory):\n",
    "  print(\"File ditemukan\")\n",
    "else:\n",
    "    print(\"tidak ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n0     63    1   3       145   233    1        0      150      0      2.3   \n1     37    1   2       130   250    0        1      187      0      3.5   \n2     41    0   1       130   204    0        0      172      0      1.4   \n3     56    1   1       120   236    0        1      178      0      0.8   \n4     57    0   0       120   354    0        1      163      1      0.6   \n..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n298   57    0   0       140   241    0        1      123      1      0.2   \n299   45    1   3       110   264    0        1      132      0      1.2   \n300   68    1   0       144   193    1        1      141      0      3.4   \n301   57    1   0       130   131    0        1      115      1      1.2   \n302   57    0   1       130   236    0        0      174      0      0.0   \n\n     slope  ca  thal  target  \n0        0   0     1       1  \n1        0   0     2       1  \n2        2   0     2       1  \n3        2   0     2       1  \n4        2   0     2       1  \n..     ...  ..   ...     ...  \n298      1   0     3       0  \n299      1   0     3       0  \n300      1   2     3       0  \n301      1   1     3       0  \n302      1   1     2       0  \n\n[303 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>1</td>\n      <td>3</td>\n      <td>145</td>\n      <td>233</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>2.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>236</td>\n      <td>0</td>\n      <td>1</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>1</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>140</td>\n      <td>241</td>\n      <td>0</td>\n      <td>1</td>\n      <td>123</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>45</td>\n      <td>1</td>\n      <td>3</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>1</td>\n      <td>132</td>\n      <td>0</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>68</td>\n      <td>1</td>\n      <td>0</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>1</td>\n      <td>141</td>\n      <td>0</td>\n      <td>3.4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>57</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>1</td>\n      <td>115</td>\n      <td>1</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>57</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>0</td>\n      <td>174</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>303 rows Ã— 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dataset = pd.read_csv(directory)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              age         sex          cp    trestbps        chol         fbs  \\\ncount  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \nmean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \nstd      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \nmin     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \nmax     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n\n          restecg     thalach       exang     oldpeak       slope          ca  \\\ncount  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \nmean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \nstd      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \nmin      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \nmax      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n\n             thal      target  \ncount  303.000000  303.000000  \nmean     2.313531    0.544554  \nstd      0.612277    0.498835  \nmin      0.000000    0.000000  \n25%      2.000000    0.000000  \n50%      2.000000    1.000000  \n75%      3.000000    1.000000  \nmax      3.000000    1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>54.366337</td>\n      <td>0.683168</td>\n      <td>0.966997</td>\n      <td>131.623762</td>\n      <td>246.264026</td>\n      <td>0.148515</td>\n      <td>0.528053</td>\n      <td>149.646865</td>\n      <td>0.326733</td>\n      <td>1.039604</td>\n      <td>1.399340</td>\n      <td>0.729373</td>\n      <td>2.313531</td>\n      <td>0.544554</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.082101</td>\n      <td>0.466011</td>\n      <td>1.032052</td>\n      <td>17.538143</td>\n      <td>51.830751</td>\n      <td>0.356198</td>\n      <td>0.525860</td>\n      <td>22.905161</td>\n      <td>0.469794</td>\n      <td>1.161075</td>\n      <td>0.616226</td>\n      <td>1.022606</td>\n      <td>0.612277</td>\n      <td>0.498835</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>29.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>94.000000</td>\n      <td>126.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>71.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>47.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>120.000000</td>\n      <td>211.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>133.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>55.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>130.000000</td>\n      <td>240.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>153.000000</td>\n      <td>0.000000</td>\n      <td>0.800000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>61.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>140.000000</td>\n      <td>274.500000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>166.000000</td>\n      <td>1.000000</td>\n      <td>1.600000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>77.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>200.000000</td>\n      <td>564.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>202.000000</td>\n      <td>1.000000</td>\n      <td>6.200000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trestbps  303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalach   303 non-null    int64  \n 8   exang     303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slope     303 non-null    int64  \n 11  ca        303 non-null    int64  \n 12  thal      303 non-null    int64  \n 13  target    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\n"
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "source": [
    "## Correlation matrix of the features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      column  Correlation with target\n0     target                 1.000000\n1         cp                 0.433798\n2    thalach                 0.421741\n3      slope                 0.345877\n4    restecg                 0.137230\n5        fbs                -0.028046\n6       chol                -0.085239\n7   trestbps                -0.144931\n8        age                -0.225439\n9        sex                -0.280937\n10      thal                -0.344029\n11        ca                -0.391724\n12   oldpeak                -0.430696\n13     exang                -0.436757",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column</th>\n      <th>Correlation with target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>target</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cp</td>\n      <td>0.433798</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thalach</td>\n      <td>0.421741</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>slope</td>\n      <td>0.345877</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>restecg</td>\n      <td>0.137230</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fbs</td>\n      <td>-0.028046</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>chol</td>\n      <td>-0.085239</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>trestbps</td>\n      <td>-0.144931</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>age</td>\n      <td>-0.225439</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sex</td>\n      <td>-0.280937</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>thal</td>\n      <td>-0.344029</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ca</td>\n      <td>-0.391724</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>oldpeak</td>\n      <td>-0.430696</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>exang</td>\n      <td>-0.436757</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "correlation_matrix = dataset.corr()\n",
    "corr = correlation_matrix['target'].sort_values(ascending=False)\n",
    "correlation_dataframe = pd.DataFrame({'column': corr.index,\n",
    "                 'Correlation with target': corr.values})\n",
    "correlation_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset['target'] #--> Target Prediksi\n",
    "feature_used = dataset.drop(['target',],axis=1) #--> Fitur yang digunakan adalah selain fitur 'target'\n",
    "\n",
    "# Menggunakan 2 metode scalling, Standard dan Power Transform dengan yeo-jhonson\n",
    "scaler_1 = StandardScaler(with_std=True,with_mean=True)\n",
    "scaler_2 = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "x_1 = scaler_1.fit_transform(feature_used)\n",
    "x_2 = scaler_2.fit_transform(feature_used)\n",
    "\n",
    "data_x_1 = pd.DataFrame(x_1, columns=feature_used.columns)\n",
    "data_x_2 = pd.DataFrame(x_2, columns=feature_used.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(obj, Predict, Feature2, Label2):\n",
    "    print('Accuracy   on test set: {:.3f}'.format(obj.score(Feature2, Label2)))\n",
    "    print('F1_score   on test set: {:.3f}'.format(f1_score(Label2, Predict, average='macro')))\n",
    "    print('Precision  on test set: {:.3f}'.format(precision_score(Label2, Predict, average='macro')))\n",
    "    print('Recall     on test set: {:.3f}'.format(recall_score(Label2, Predict, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data 80-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.866\nPrecision  on test set: 0.863\nRecall     on test set: 0.871\n-----------------------------------------------------------------------------------------------------------------------------\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.918\nF1_score   on test set: 0.915\nPrecision  on test set: 0.918\nRecall     on test set: 0.912\n"
    }
   ],
   "source": [
    "# Training menggunakan data hasil scalling dengan Standard Scaler\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "logReg = LogisticRegression(tol=0.001)\n",
    "logReg.fit(feature_train,label_train)\n",
    "cross_val= cross_val_score(logReg, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= logReg.predict(feature_test)\n",
    "scores(logReg, prediction, feature_test, label_test)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "# Training menggunakan data hasil scalling dengan Power Transform Scaler\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20,random_state=4)\n",
    "logReg = LogisticRegression(tol=0.001)\n",
    "logReg.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(logReg, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= logReg.predict(feature_test)\n",
    "scores(logReg, prediction, feature_test, label_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.866\nPrecision  on test set: 0.863\nRecall     on test set: 0.871\n-----------------------------------------------------------------------------------------------------------------------------\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.918\nF1_score   on test set: 0.915\nPrecision  on test set: 0.918\nRecall     on test set: 0.912\n"
    }
   ],
   "source": [
    "print(\"Menggunakan Standard Scaler\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "mlc = MLPClassifier(hidden_layer_sizes=50, activation='identity',solver='lbfgs',batch_size='auto', learning_rate_init=0.0001, max_iter=10000,early_stopping=False)\n",
    "mlc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(mlc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= mlc.predict(feature_test)\n",
    "scores(mlc, prediction, feature_test, label_test)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "mlc= MLPClassifier(hidden_layer_sizes=50, activation='identity',solver='lbfgs',batch_size='auto', learning_rate_init=0.0001, max_iter=10000, early_stopping=False)\n",
    "mlc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(mlc, feature_train, label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= mlc.predict(feature_test)\n",
    "scores(mlc, prediction, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nUntuk Estimator 5\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.802\nPrecision  on test set: 0.805\nRecall     on test set: 0.815\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.787\nF1_score   on test set: 0.778\nPrecision  on test set: 0.780\nRecall     on test set: 0.777\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 10\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.918\nF1_score   on test set: 0.916\nPrecision  on test set: 0.914\nRecall     on test set: 0.918\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.867\nPrecision  on test set: 0.865\nRecall     on test set: 0.877\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 20\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.836\nF1_score   on test set: 0.834\nPrecision  on test set: 0.832\nRecall     on test set: 0.843\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.867\nPrecision  on test set: 0.865\nRecall     on test set: 0.877\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 30\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.934\nF1_score   on test set: 0.933\nPrecision  on test set: 0.930\nRecall     on test set: 0.938\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.852\nF1_score   on test set: 0.848\nPrecision  on test set: 0.847\nRecall     on test set: 0.851\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 50\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.881\nPrecision  on test set: 0.883\nRecall     on test set: 0.878\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.836\nF1_score   on test set: 0.834\nPrecision  on test set: 0.832\nRecall     on test set: 0.843\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 75\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.867\nPrecision  on test set: 0.865\nRecall     on test set: 0.877\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.852\nF1_score   on test set: 0.850\nPrecision  on test set: 0.847\nRecall     on test set: 0.857\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 100\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.866\nPrecision  on test set: 0.863\nRecall     on test set: 0.871\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.882\nPrecision  on test set: 0.880\nRecall     on test set: 0.884\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 150\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.882\nPrecision  on test set: 0.880\nRecall     on test set: 0.884\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.882\nPrecision  on test set: 0.880\nRecall     on test set: 0.884\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "estimator =[5,10,20,30,50,75,100,150]\n",
    "for i in estimator:\n",
    "    print(\"Menggunakan Standard Scaler\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Untuk Estimator \"+str(i))\n",
    "    rfc = RandomForestClassifier(i,criterion='gini',bootstrap=True,class_weight='balanced_subsample')\n",
    "    rfc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(rfc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= rfc.predict(feature_test)\n",
    "    scores(rfc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "\n",
    "    print(\"Menggunakan PowerTransform Scaller\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "    rfc = RandomForestClassifier(i,criterion='gini',bootstrap=True,class_weight='balanced_subsample')\n",
    "    rfc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(rfc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= rfc.predict(feature_test)\n",
    "    scores(rfc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gausssian-Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.881\nPrecision  on test set: 0.883\nRecall     on test set: 0.878\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.902\nF1_score   on test set: 0.898\nPrecision  on test set: 0.898\nRecall     on test set: 0.898\n\n"
    }
   ],
   "source": [
    "print(\"Menggunakan Standard Scaler\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, max_iter_predict=100, optimizer='fmin_l_bfgs_b')\n",
    "gpc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gpc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gpc.predict(feature_test)\n",
    "scores(gpc, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan Power Transform Scaler\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, max_iter_predict=100,optimizer='fmin_l_bfgs_b')\n",
    "gpc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gpc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gpc.predict(feature_test)\n",
    "scores(gpc, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan nilai k : 1\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.770\nF1_score   on test set: 0.767\nPrecision  on test set: 0.767\nRecall     on test set: 0.775\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.820\nF1_score   on test set: 0.819\nPrecision  on test set: 0.834\nRecall     on test set: 0.841\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 3\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.799\nPrecision  on test set: 0.797\nRecall     on test set: 0.803\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.802\nPrecision  on test set: 0.805\nRecall     on test set: 0.815\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 5\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.799\nPrecision  on test set: 0.797\nRecall     on test set: 0.803\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.852\nF1_score   on test set: 0.851\nPrecision  on test set: 0.851\nRecall     on test set: 0.863\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 7\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.836\nF1_score   on test set: 0.832\nPrecision  on test set: 0.830\nRecall     on test set: 0.837\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.867\nPrecision  on test set: 0.865\nRecall     on test set: 0.877\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 9\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.799\nPrecision  on test set: 0.797\nRecall     on test set: 0.803\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.869\nF1_score   on test set: 0.867\nPrecision  on test set: 0.865\nRecall     on test set: 0.877\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 11\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.799\nPrecision  on test set: 0.797\nRecall     on test set: 0.803\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.885\nPrecision  on test set: 0.891\nRecall     on test set: 0.903\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 13\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.820\nF1_score   on test set: 0.815\nPrecision  on test set: 0.813\nRecall     on test set: 0.817\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.902\nF1_score   on test set: 0.901\nPrecision  on test set: 0.903\nRecall     on test set: 0.917\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 15\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.787\nF1_score   on test set: 0.781\nPrecision  on test set: 0.780\nRecall     on test set: 0.783\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.902\nF1_score   on test set: 0.900\nPrecision  on test set: 0.898\nRecall     on test set: 0.911\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "numbers = 16\n",
    "for i in range(numbers):\n",
    "    if i %2 != 0 :\n",
    "        print(\"Menggunakan nilai k : \"+str(i))\n",
    "        print(\"Menggunakan Standard Scaler\")\n",
    "        feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "        knn = KNeighborsClassifier(i,weights='distance',algorithm='auto')\n",
    "        knn.fit(feature_train,label_train)\n",
    "        cross_val_= cross_val_score(knn, feature_train,label_train, cv=5)\n",
    "        print(\"Cross Validation Score : \"+str(cross_val))\n",
    "        prediction= knn.predict(feature_test)\n",
    "        scores(knn, prediction, feature_test, label_test)\n",
    "        print()\n",
    "\n",
    "        print(\"Menggunakan Power Transform Scaler\")\n",
    "        feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "        knn = KNeighborsClassifier(i,weights='distance',algorithm='auto')\n",
    "        knn.fit(feature_train,label_train)\n",
    "        cross_val_= cross_val_score(knn, feature_train,label_train, cv=5)\n",
    "        print(\"Cross Validation Score : \"+str(cross_val))\n",
    "        prediction= knn.predict(feature_test)\n",
    "        scores(knn, prediction, feature_test, label_test)\n",
    "        print()\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nUntuk Estimator 5\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 10\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 20\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 30\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 50\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 75\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 100\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 150\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "estimator =[5,10,20,30,50,75,100,150]\n",
    "for i in estimator:\n",
    "    print(\"Menggunakan Standard Scaler\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Untuk Estimator \"+str(i))\n",
    "    adc = AdaBoostClassifier(n_estimators=i,learning_rate=0.0001, random_state=1,algorithm='SAMME')\n",
    "    adc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(adc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= adc.predict(feature_test)\n",
    "    scores(adc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "\n",
    "    print(\"Menggunakan PowerTransform Scaller\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "    adc = AdaBoostClassifier(n_estimators=i,learning_rate=0.0001, random_state=1,algorithm='SAMME')\n",
    "    adc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(adc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= adc.predict(feature_test)\n",
    "    scores(adc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.799\nPrecision  on test set: 0.797\nRecall     on test set: 0.803\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.797\nPrecision  on test set: 0.797\nRecall     on test set: 0.797\n\n"
    }
   ],
   "source": [
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "dtc=DecisionTreeClassifier(criterion='entropy',splitter='best',class_weight='balanced')\n",
    "dtc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(dtc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= dtc.predict(feature_test)\n",
    "scores(dtc, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "dtc = DecisionTreeClassifier(criterion='entropy',splitter='best',class_weight='balanced')\n",
    "dtc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(dtc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= dtc.predict(feature_test)\n",
    "scores(dtc, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.836\nF1_score   on test set: 0.828\nPrecision  on test set: 0.834\nRecall     on test set: 0.824\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.882\nPrecision  on test set: 0.880\nRecall     on test set: 0.884\n\n"
    }
   ],
   "source": [
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gnb, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gnb.predict(feature_test)\n",
    "scores(gnb, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gnb, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gnb.predict(feature_test)\n",
    "scores(gnb, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan kernel : poly\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.770\nF1_score   on test set: 0.767\nPrecision  on test set: 0.767\nRecall     on test set: 0.775\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.770\nF1_score   on test set: 0.770\nPrecision  on test set: 0.780\nRecall     on test set: 0.787\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan kernel : linear\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.902\nF1_score   on test set: 0.899\nPrecision  on test set: 0.897\nRecall     on test set: 0.904\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.885\nF1_score   on test set: 0.882\nPrecision  on test set: 0.880\nRecall     on test set: 0.884\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan kernel : rbf\nMenggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.803\nF1_score   on test set: 0.802\nPrecision  on test set: 0.805\nRecall     on test set: 0.815\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.770\nF1_score   on test set: 0.770\nPrecision  on test set: 0.780\nRecall     on test set: 0.787\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "kernels=['poly','linear','rbf']\n",
    "for i, value in enumerate(kernels):\n",
    "    print(\"Menggunakan kernel : \"+str(value))\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Menggunakan Standard Scaler\")\n",
    "    svl=SVC(kernel= value, C=1000,gamma='scale',tol=0.0001,class_weight='balanced',degree=3,coef0=1)\n",
    "    svl.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(svl, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= svl.predict(feature_test)\n",
    "    scores(svl, prediction, feature_test, label_test)\n",
    "    print()\n",
    "\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Menggunakan Power Transform Scaler\")\n",
    "    svl=SVC(kernel= value, C=1000,gamma='scale',tol=0.0001,class_weight='balanced',degree=3,coef0=1)\n",
    "    svl.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(svl, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= svl.predict(feature_test)\n",
    "    scores(svl, prediction, feature_test, label_test)\n",
    "    print()\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.738\nF1_score   on test set: 0.866\nPrecision  on test set: 0.863\nRecall     on test set: 0.871\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.71428571 0.81632653 0.85416667 0.79166667 0.875     ]\nAccuracy   on test set: 0.770\nF1_score   on test set: 0.898\nPrecision  on test set: 0.898\nRecall     on test set: 0.898\n\n"
    }
   ],
   "source": [
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "linearsvc = LinearSVC(class_weight='balanced',max_iter=1000)\n",
    "linearsvc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(linearsvc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= linearsvc.predict(feature_test)\n",
    "scores(svl, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Power Transform Scaler\")\n",
    "linearsvc = LinearSVC(class_weight='balanced',max_iter=1000)\n",
    "linearsvc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(linearsvc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= linearsvc.predict(feature_test)\n",
    "scores(svl, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kesimpulan\n",
    "\n",
    "Pada percobaan diatas, menggunakan dataset 'Diabetes'. digunakan 10 algoritma untuk mengklasifikasi data menjadi 2 kelas\n",
    "yaitu 0 = 'Tidak terkena Diabetes' , 1 = 'Terkena Diabetes' .\n",
    "\n",
    "Dataset tersebut di-scaling dengan 2 scaler, yaitu 'Standard scaler' dan 'Power Transform'.\n",
    "\n",
    "* Algoritma LogisticRegression mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.869 dan 0.863 , Power Transform : 0.918 dan 0,918\n",
    "\n",
    "* Algoritma MLP Classifier mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.869 dan 0.863 , Power Transform : 0.918 dan 0.918\n",
    "\n",
    "* Algoritma RandomForest Classifier mendapat nilai akurasi dan precision\n",
    "yang terbaik ada pada nilai estimator 30, dengan :\n",
    "\n",
    "    Standard scaler : 0.932 dan 0.930 , Power Transform : 0.852 dan 0.847\n",
    "\n",
    "* Algoritma KNN Classifier mendapat nilai akurasi dan precision\n",
    "yang terbaik ada pada nilai k = 13, dengan :\n",
    "\n",
    "    Standard scaler : 0.820 dan 0.15 , Power Transform : 0.903 dan 0.902\n",
    "\n",
    "* Algoritma Adaboost Classifier mendapat nilai akurasi dan precision\n",
    "yang terbaik ada pada nilai estimator 5, dengan :\n",
    "\n",
    "    Standard scaler : 0.803 dan 0.797 , Power Transform : 0.803 dan 0.797\n",
    "\n",
    "* Algoritma Decision Tree Classifier mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.803 dan 0.797 , Power Transform : 0.803 dan 0.797\n",
    "\n",
    "* Algoritma Gaussian Naive-Bayes mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.830 dan 0.834 , Power Transform : 0.885 dan 0.880\n",
    "\n",
    "* Algoritma SVC yang mendapat nilai terbaik menggunakan kernel 'linear'\n",
    "dengan nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.902 dan 0.897 , Power Transform : 0.885 dan 0.880\n",
    "\n",
    "* Algoritma LinearSVC mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.738 dan 0.863 , Power Transform : 0.770, 0.898\n",
    "\n",
    "Dari hasil percobaan diatas, algoritma RandomForest Classifier, menggunakan data yang\n",
    "di-scaling dengan Standard scaler mendapat hasil akurasi yang terbaik sebesar 0.932\n",
    "\n",
    "Maka, dapat disimpulkan bahwa penggunaan Standard scaler pada dataset ini sudah mencukupi\n",
    "untuk bisa mendapat nilai akurasi dan precision yang cukup baik pada kebanyakan algoritma yang digunakan. \n",
    "Tapi, dalam beberapa algoritma akan lebih baik hasilnya jika menggunakan Power Transform scaler.\n",
    "\n",
    "Hasil klasifikasi yang didapatkan oleh model algoritma dapat dipengaruhi oleh banyak faktor,\n",
    "salah satunya adalah penggunaan Hyperparameter pada tiap algoritma dan keseimbangan jumlah sebaran data pada label.\n",
    "jika seimbang, maka pengklasifikasian data akan terjadi merata / tidak condong pada 1 kelas."
   ]
  }
 ]
}