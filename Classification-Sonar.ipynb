{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599802932268",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Sonar\n",
    "\n",
    "Some classification problem on sonar.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imblearn\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File ditemukan\n"
    }
   ],
   "source": [
    "directory = os.path.join('D:/Bootcamp ML - Mada/classification dataset/', 'sonar.csv')\n",
    "if os.path.isfile(directory):\n",
    "  print(\"File ditemukan\")\n",
    "else:\n",
    "    print(\"tidak ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         V1      V2      V3      V4      V5      V6      V7      V8      V9  \\\n0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n\n        V10  ...     V52     V53     V54     V55     V56     V57     V58  \\\n0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n\n        V59     V60  Class  \n0    0.0090  0.0032      1  \n1    0.0052  0.0044      1  \n2    0.0095  0.0078      1  \n3    0.0040  0.0117      1  \n4    0.0107  0.0094      1  \n..      ...     ...    ...  \n203  0.0193  0.0157      0  \n204  0.0062  0.0067      0  \n205  0.0077  0.0031      0  \n206  0.0036  0.0048      0  \n207  0.0061  0.0115      0  \n\n[208 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>...</th>\n      <th>V52</th>\n      <th>V53</th>\n      <th>V54</th>\n      <th>V55</th>\n      <th>V56</th>\n      <th>V57</th>\n      <th>V58</th>\n      <th>V59</th>\n      <th>V60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.0323</td>\n      <td>0.0101</td>\n      <td>0.0298</td>\n      <td>0.0564</td>\n      <td>0.0760</td>\n      <td>0.0958</td>\n      <td>0.0990</td>\n      <td>0.1018</td>\n      <td>0.1030</td>\n      <td>0.2154</td>\n      <td>...</td>\n      <td>0.0061</td>\n      <td>0.0093</td>\n      <td>0.0135</td>\n      <td>0.0063</td>\n      <td>0.0063</td>\n      <td>0.0034</td>\n      <td>0.0032</td>\n      <td>0.0062</td>\n      <td>0.0067</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.0303</td>\n      <td>0.0353</td>\n      <td>0.0490</td>\n      <td>0.0608</td>\n      <td>0.0167</td>\n      <td>0.1354</td>\n      <td>0.1465</td>\n      <td>0.1123</td>\n      <td>0.1945</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0086</td>\n      <td>0.0046</td>\n      <td>0.0126</td>\n      <td>0.0036</td>\n      <td>0.0035</td>\n      <td>0.0034</td>\n      <td>0.0079</td>\n      <td>0.0036</td>\n      <td>0.0048</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.0260</td>\n      <td>0.0363</td>\n      <td>0.0136</td>\n      <td>0.0272</td>\n      <td>0.0214</td>\n      <td>0.0338</td>\n      <td>0.0655</td>\n      <td>0.1400</td>\n      <td>0.1843</td>\n      <td>0.2354</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0129</td>\n      <td>0.0047</td>\n      <td>0.0039</td>\n      <td>0.0061</td>\n      <td>0.0040</td>\n      <td>0.0036</td>\n      <td>0.0061</td>\n      <td>0.0115</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows × 61 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dataset = pd.read_csv(directory)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 208 entries, 0 to 207\nData columns (total 61 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   V1      208 non-null    float64\n 1   V2      208 non-null    float64\n 2   V3      208 non-null    float64\n 3   V4      208 non-null    float64\n 4   V5      208 non-null    float64\n 5   V6      208 non-null    float64\n 6   V7      208 non-null    float64\n 7   V8      208 non-null    float64\n 8   V9      208 non-null    float64\n 9   V10     208 non-null    float64\n 10  V11     208 non-null    float64\n 11  V12     208 non-null    float64\n 12  V13     208 non-null    float64\n 13  V14     208 non-null    float64\n 14  V15     208 non-null    float64\n 15  V16     208 non-null    float64\n 16  V17     208 non-null    float64\n 17  V18     208 non-null    float64\n 18  V19     208 non-null    float64\n 19  V20     208 non-null    float64\n 20  V21     208 non-null    float64\n 21  V22     208 non-null    float64\n 22  V23     208 non-null    float64\n 23  V24     208 non-null    float64\n 24  V25     208 non-null    float64\n 25  V26     208 non-null    float64\n 26  V27     208 non-null    float64\n 27  V28     208 non-null    float64\n 28  V29     208 non-null    float64\n 29  V30     208 non-null    float64\n 30  V31     208 non-null    float64\n 31  V32     208 non-null    float64\n 32  V33     208 non-null    float64\n 33  V34     208 non-null    float64\n 34  V35     208 non-null    float64\n 35  V36     208 non-null    float64\n 36  V37     208 non-null    float64\n 37  V38     208 non-null    float64\n 38  V39     208 non-null    float64\n 39  V40     208 non-null    float64\n 40  V41     208 non-null    float64\n 41  V42     208 non-null    float64\n 42  V43     208 non-null    float64\n 43  V44     208 non-null    float64\n 44  V45     208 non-null    float64\n 45  V46     208 non-null    float64\n 46  V47     208 non-null    float64\n 47  V48     208 non-null    float64\n 48  V49     208 non-null    float64\n 49  V50     208 non-null    float64\n 50  V51     208 non-null    float64\n 51  V52     208 non-null    float64\n 52  V53     208 non-null    float64\n 53  V54     208 non-null    float64\n 54  V55     208 non-null    float64\n 55  V56     208 non-null    float64\n 56  V57     208 non-null    float64\n 57  V58     208 non-null    float64\n 58  V59     208 non-null    float64\n 59  V60     208 non-null    float64\n 60  Class   208 non-null    int64  \ndtypes: float64(60), int64(1)\nmemory usage: 99.2 KB\n"
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    111\n1     97\nName: Class, dtype: int64\n"
    }
   ],
   "source": [
    "print(dataset['Class'].value_counts())"
   ]
  },
  {
   "source": [
    "## Correlation of the features on dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   column  Correlation with target\n0   Class                 1.000000\n1     V36                 0.269151\n2     V35                 0.227670\n3     V37                 0.209055\n4     V34                 0.172010\n..    ...                      ...\n56    V45                -0.339406\n57    V10                -0.341142\n58    V49                -0.351312\n59    V12                -0.392245\n60    V11                -0.432855\n\n[61 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column</th>\n      <th>Correlation with target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Class</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>V36</td>\n      <td>0.269151</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>V35</td>\n      <td>0.227670</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>V37</td>\n      <td>0.209055</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>V34</td>\n      <td>0.172010</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>V45</td>\n      <td>-0.339406</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>V10</td>\n      <td>-0.341142</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>V49</td>\n      <td>-0.351312</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>V12</td>\n      <td>-0.392245</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>V11</td>\n      <td>-0.432855</td>\n    </tr>\n  </tbody>\n</table>\n<p>61 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "correlation_matrix = dataset.corr()\n",
    "corr = correlation_matrix['Class'].sort_values(ascending=False)\n",
    "correlation_dataframe = pd.DataFrame({'column': corr.index,\n",
    "                 'Correlation with target': corr.values})\n",
    "correlation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               V1          V2          V3          V4          V5          V6  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \nstd      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \nmin      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \nmax      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n\n               V7          V8          V9         V10  ...         V52  \\\ncount  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \nmean     0.121747    0.134799    0.178003    0.208259  ...    0.013420   \nstd      0.061788    0.085152    0.118387    0.134416  ...    0.009634   \nmin      0.003300    0.005500    0.007500    0.011300  ...    0.000800   \n25%      0.080900    0.080425    0.097025    0.111275  ...    0.007275   \n50%      0.106950    0.112100    0.152250    0.182400  ...    0.011400   \n75%      0.154000    0.169600    0.233425    0.268700  ...    0.016725   \nmax      0.372900    0.459000    0.682800    0.710600  ...    0.070900   \n\n              V53         V54         V55         V56         V57         V58  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.010709    0.010941    0.009290    0.008222    0.007820    0.007949   \nstd      0.007060    0.007301    0.007088    0.005736    0.005785    0.006470   \nmin      0.000500    0.001000    0.000600    0.000400    0.000300    0.000300   \n25%      0.005075    0.005375    0.004150    0.004400    0.003700    0.003600   \n50%      0.009550    0.009300    0.007500    0.006850    0.005950    0.005800   \n75%      0.014900    0.014500    0.012100    0.010575    0.010425    0.010350   \nmax      0.039000    0.035200    0.044700    0.039400    0.035500    0.044000   \n\n              V59         V60       Class  \ncount  208.000000  208.000000  208.000000  \nmean     0.007941    0.006507    0.466346  \nstd      0.006181    0.005031    0.500070  \nmin      0.000100    0.000600    0.000000  \n25%      0.003675    0.003100    0.000000  \n50%      0.006400    0.005300    0.000000  \n75%      0.010325    0.008525    1.000000  \nmax      0.036400    0.043900    1.000000  \n\n[8 rows x 61 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>...</th>\n      <th>V52</th>\n      <th>V53</th>\n      <th>V54</th>\n      <th>V55</th>\n      <th>V56</th>\n      <th>V57</th>\n      <th>V58</th>\n      <th>V59</th>\n      <th>V60</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>...</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.029164</td>\n      <td>0.038437</td>\n      <td>0.043832</td>\n      <td>0.053892</td>\n      <td>0.075202</td>\n      <td>0.104570</td>\n      <td>0.121747</td>\n      <td>0.134799</td>\n      <td>0.178003</td>\n      <td>0.208259</td>\n      <td>...</td>\n      <td>0.013420</td>\n      <td>0.010709</td>\n      <td>0.010941</td>\n      <td>0.009290</td>\n      <td>0.008222</td>\n      <td>0.007820</td>\n      <td>0.007949</td>\n      <td>0.007941</td>\n      <td>0.006507</td>\n      <td>0.466346</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.022991</td>\n      <td>0.032960</td>\n      <td>0.038428</td>\n      <td>0.046528</td>\n      <td>0.055552</td>\n      <td>0.059105</td>\n      <td>0.061788</td>\n      <td>0.085152</td>\n      <td>0.118387</td>\n      <td>0.134416</td>\n      <td>...</td>\n      <td>0.009634</td>\n      <td>0.007060</td>\n      <td>0.007301</td>\n      <td>0.007088</td>\n      <td>0.005736</td>\n      <td>0.005785</td>\n      <td>0.006470</td>\n      <td>0.006181</td>\n      <td>0.005031</td>\n      <td>0.500070</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001500</td>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.010200</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.007500</td>\n      <td>0.011300</td>\n      <td>...</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000300</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.013350</td>\n      <td>0.016450</td>\n      <td>0.018950</td>\n      <td>0.024375</td>\n      <td>0.038050</td>\n      <td>0.067025</td>\n      <td>0.080900</td>\n      <td>0.080425</td>\n      <td>0.097025</td>\n      <td>0.111275</td>\n      <td>...</td>\n      <td>0.007275</td>\n      <td>0.005075</td>\n      <td>0.005375</td>\n      <td>0.004150</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003675</td>\n      <td>0.003100</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.022800</td>\n      <td>0.030800</td>\n      <td>0.034300</td>\n      <td>0.044050</td>\n      <td>0.062500</td>\n      <td>0.092150</td>\n      <td>0.106950</td>\n      <td>0.112100</td>\n      <td>0.152250</td>\n      <td>0.182400</td>\n      <td>...</td>\n      <td>0.011400</td>\n      <td>0.009550</td>\n      <td>0.009300</td>\n      <td>0.007500</td>\n      <td>0.006850</td>\n      <td>0.005950</td>\n      <td>0.005800</td>\n      <td>0.006400</td>\n      <td>0.005300</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.035550</td>\n      <td>0.047950</td>\n      <td>0.057950</td>\n      <td>0.064500</td>\n      <td>0.100275</td>\n      <td>0.134125</td>\n      <td>0.154000</td>\n      <td>0.169600</td>\n      <td>0.233425</td>\n      <td>0.268700</td>\n      <td>...</td>\n      <td>0.016725</td>\n      <td>0.014900</td>\n      <td>0.014500</td>\n      <td>0.012100</td>\n      <td>0.010575</td>\n      <td>0.010425</td>\n      <td>0.010350</td>\n      <td>0.010325</td>\n      <td>0.008525</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.137100</td>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>...</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 61 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset['Class'] #--> Target Prediksi\n",
    "feature_used = dataset.drop(['Class',],axis=1) #--> Fitur yang digunakan adalah selain fitur 'target'\n",
    "\n",
    "# Menggunakan 2 metode scalling, Standard dan Power Transform dengan yeo-jhonson\n",
    "scaler_1 = StandardScaler(with_std=True,with_mean=True)\n",
    "scaler_2 = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "x_1 = scaler_1.fit_transform(feature_used)\n",
    "x_2 = scaler_2.fit_transform(feature_used)\n",
    "\n",
    "data_x_1 = pd.DataFrame(x_1, columns=feature_used.columns)\n",
    "data_x_2 = pd.DataFrame(x_2, columns=feature_used.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(obj, Predict, Feature2, Label2):\n",
    "    print('Accuracy   on test set: {:.3f}'.format(obj.score(Feature2, Label2)))\n",
    "    print('F1_score   on test set: {:.3f}'.format(f1_score(Label2, Predict, average='macro')))\n",
    "    print('Precision  on test set: {:.3f}'.format(precision_score(Label2, Predict, average='macro')))\n",
    "    print('Recall     on test set: {:.3f}'.format(recall_score(Label2, Predict, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data 80-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.810\nF1_score   on test set: 0.802\nPrecision  on test set: 0.802\nRecall     on test set: 0.802\n-----------------------------------------------------------------------------------------------------------------------------\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.810\nF1_score   on test set: 0.802\nPrecision  on test set: 0.802\nRecall     on test set: 0.802\n"
    }
   ],
   "source": [
    "# Training menggunakan data hasil scalling dengan Standard Scaler\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(feature_train,label_train)\n",
    "cross_val= cross_val_score(logReg, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= logReg.predict(feature_test)\n",
    "scores(logReg, prediction, feature_test, label_test)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "# Training menggunakan data hasil scalling dengan Power Transform Scaler\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20,random_state=4)\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(logReg, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= logReg.predict(feature_test)\n",
    "scores(logReg, prediction, feature_test, label_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.896\nPrecision  on test set: 0.931\nRecall     on test set: 0.882\n-----------------------------------------------------------------------------------------------------------------------------\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.899\nPrecision  on test set: 0.911\nRecall     on test set: 0.892\n"
    }
   ],
   "source": [
    "print(\"Menggunakan Standard Scaler\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "mlc = MLPClassifier(hidden_layer_sizes=100, activation='relu',solver='lbfgs',batch_size='auto', learning_rate_init=0.0001, max_iter=10000,early_stopping=False)\n",
    "mlc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(mlc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= mlc.predict(feature_test)\n",
    "scores(mlc, prediction, feature_test, label_test)\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "mlc= MLPClassifier(hidden_layer_sizes=100, activation='relu',solver='lbfgs',batch_size='auto', learning_rate_init=0.0001, max_iter=10000, early_stopping=False)\n",
    "mlc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(mlc, feature_train, label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= mlc.predict(feature_test)\n",
    "scores(mlc, prediction, feature_test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nUntuk Estimator 5\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.825\nPrecision  on test set: 0.829\nRecall     on test set: 0.822\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.690\nF1_score   on test set: 0.658\nPrecision  on test set: 0.683\nRecall     on test set: 0.655\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 10\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.810\nF1_score   on test set: 0.802\nPrecision  on test set: 0.802\nRecall     on test set: 0.802\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.786\nF1_score   on test set: 0.770\nPrecision  on test set: 0.786\nRecall     on test set: 0.764\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 20\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.810\nF1_score   on test set: 0.793\nPrecision  on test set: 0.820\nRecall     on test set: 0.784\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.825\nPrecision  on test set: 0.829\nRecall     on test set: 0.822\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 30\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.849\nPrecision  on test set: 0.859\nRecall     on test set: 0.842\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.809\nPrecision  on test set: 0.891\nRecall     on test set: 0.794\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 50\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.849\nPrecision  on test set: 0.859\nRecall     on test set: 0.842\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.849\nPrecision  on test set: 0.859\nRecall     on test set: 0.842\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 75\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.849\nPrecision  on test set: 0.859\nRecall     on test set: 0.842\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.849\nPrecision  on test set: 0.859\nRecall     on test set: 0.842\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 100\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.881\nF1_score   on test set: 0.872\nPrecision  on test set: 0.893\nRecall     on test set: 0.862\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.881\nF1_score   on test set: 0.872\nPrecision  on test set: 0.893\nRecall     on test set: 0.862\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 150\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.849\nPrecision  on test set: 0.859\nRecall     on test set: 0.842\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.839\nPrecision  on test set: 0.903\nRecall     on test set: 0.824\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "estimator =[5,10,20,30,50,75,100,150]\n",
    "for i in estimator:\n",
    "    print(\"Menggunakan Standard Scaler\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Untuk Estimator \"+str(i))\n",
    "    rfc = RandomForestClassifier(i,criterion='gini',bootstrap=True,class_weight='balanced_subsample')\n",
    "    rfc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(rfc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= rfc.predict(feature_test)\n",
    "    scores(rfc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "\n",
    "    print(\"Menggunakan PowerTransform Scaller\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "    rfc = RandomForestClassifier(i,criterion='gini',bootstrap=True,class_weight='balanced_subsample')\n",
    "    rfc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(rfc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= rfc.predict(feature_test)\n",
    "    scores(rfc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian-Process Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.896\nPrecision  on test set: 0.931\nRecall     on test set: 0.882\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.844\nPrecision  on test set: 0.875\nRecall     on test set: 0.833\n\n"
    }
   ],
   "source": [
    "print(\"Menggunakan Standard Scaler\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, max_iter_predict=100, optimizer='fmin_l_bfgs_b')\n",
    "gpc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gpc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gpc.predict(feature_test)\n",
    "scores(gpc, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan Power Transform Scaler\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, max_iter_predict=100,optimizer='fmin_l_bfgs_b')\n",
    "gpc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gpc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gpc.predict(feature_test)\n",
    "scores(gpc, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan nilai k : 1\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.786\nF1_score   on test set: 0.770\nPrecision  on test set: 0.786\nRecall     on test set: 0.764\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.821\nPrecision  on test set: 0.839\nRecall     on test set: 0.813\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 3\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.786\nF1_score   on test set: 0.763\nPrecision  on test set: 0.800\nRecall     on test set: 0.754\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.732\nPrecision  on test set: 0.780\nRecall     on test set: 0.725\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 5\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.816\nPrecision  on test set: 0.858\nRecall     on test set: 0.804\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.816\nPrecision  on test set: 0.858\nRecall     on test set: 0.804\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 7\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.816\nPrecision  on test set: 0.858\nRecall     on test set: 0.804\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.816\nPrecision  on test set: 0.858\nRecall     on test set: 0.804\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 9\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.844\nPrecision  on test set: 0.875\nRecall     on test set: 0.833\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.844\nPrecision  on test set: 0.875\nRecall     on test set: 0.833\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 11\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.833\nF1_score   on test set: 0.821\nPrecision  on test set: 0.839\nRecall     on test set: 0.813\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.857\nF1_score   on test set: 0.844\nPrecision  on test set: 0.875\nRecall     on test set: 0.833\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 13\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.786\nF1_score   on test set: 0.770\nPrecision  on test set: 0.786\nRecall     on test set: 0.764\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.810\nF1_score   on test set: 0.786\nPrecision  on test set: 0.842\nRecall     on test set: 0.774\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan nilai k : 15\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.786\nF1_score   on test set: 0.770\nPrecision  on test set: 0.786\nRecall     on test set: 0.764\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.786\nF1_score   on test set: 0.763\nPrecision  on test set: 0.800\nRecall     on test set: 0.754\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "numbers = 16\n",
    "for i in range(numbers):\n",
    "    if i %2 != 0 :\n",
    "        print(\"Menggunakan nilai k : \"+str(i))\n",
    "        print(\"Menggunakan Standard Scaler\")\n",
    "        feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "        knn = KNeighborsClassifier(i,weights='distance',algorithm='auto')\n",
    "        knn.fit(feature_train,label_train)\n",
    "        cross_val_= cross_val_score(knn, feature_train,label_train, cv=5)\n",
    "        print(\"Cross Validation Score : \"+str(cross_val))\n",
    "        prediction= knn.predict(feature_test)\n",
    "        scores(knn, prediction, feature_test, label_test)\n",
    "        print()\n",
    "\n",
    "        print(\"Menggunakan Power Transform Scaler\")\n",
    "        feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "        knn = KNeighborsClassifier(i,weights='distance',algorithm='auto')\n",
    "        knn.fit(feature_train,label_train)\n",
    "        cross_val_= cross_val_score(knn, feature_train,label_train, cv=5)\n",
    "        print(\"Cross Validation Score : \"+str(cross_val))\n",
    "        prediction= knn.predict(feature_test)\n",
    "        scores(knn, prediction, feature_test, label_test)\n",
    "        print()\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nUntuk Estimator 5\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 10\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 20\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 30\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 50\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 75\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.721\nPrecision  on test set: 0.808\nRecall     on test set: 0.715\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 100\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.738\nF1_score   on test set: 0.725\nPrecision  on test set: 0.728\nRecall     on test set: 0.724\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.738\nF1_score   on test set: 0.725\nPrecision  on test set: 0.728\nRecall     on test set: 0.724\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan Standard Scaler\nUntuk Estimator 150\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.738\nF1_score   on test set: 0.719\nPrecision  on test set: 0.732\nRecall     on test set: 0.714\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.738\nF1_score   on test set: 0.719\nPrecision  on test set: 0.732\nRecall     on test set: 0.714\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "estimator =[5,10,20,30,50,75,100,150]\n",
    "for i in estimator:\n",
    "    print(\"Menggunakan Standard Scaler\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Untuk Estimator \"+str(i))\n",
    "    adc = AdaBoostClassifier(n_estimators=i,learning_rate=0.001, random_state=100,algorithm='SAMME.R')\n",
    "    adc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(adc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= adc.predict(feature_test)\n",
    "    scores(adc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "\n",
    "    print(\"Menggunakan PowerTransform Scaller\")\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "    adc = AdaBoostClassifier(n_estimators=i,learning_rate=0.001, random_state=100,algorithm='SAMME.R')\n",
    "    adc.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(adc, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= adc.predict(feature_test)\n",
    "    scores(adc, prediction, feature_test, label_test)\n",
    "    print()\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.714\nF1_score   on test set: 0.704\nPrecision  on test set: 0.704\nRecall     on test set: 0.704\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.748\nPrecision  on test set: 0.756\nRecall     on test set: 0.744\n\n"
    }
   ],
   "source": [
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "dtc=DecisionTreeClassifier(criterion='entropy',splitter='best',class_weight='balanced')\n",
    "dtc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(dtc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= dtc.predict(feature_test)\n",
    "scores(dtc, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "dtc = DecisionTreeClassifier(criterion='entropy',splitter='best',class_weight='balanced')\n",
    "dtc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(dtc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= dtc.predict(feature_test)\n",
    "scores(dtc, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.619\nF1_score   on test set: 0.618\nPrecision  on test set: 0.659\nRecall     on test set: 0.652\n\nMenggunakan PowerTransform Scaller\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.643\nF1_score   on test set: 0.633\nPrecision  on test set: 0.632\nRecall     on test set: 0.634\n\n"
    }
   ],
   "source": [
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gnb, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gnb.predict(feature_test)\n",
    "scores(gnb, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "print(\"Menggunakan PowerTransform Scaller\")\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(gnb, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= gnb.predict(feature_test)\n",
    "scores(gnb, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan kernel : poly\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.899\nPrecision  on test set: 0.911\nRecall     on test set: 0.892\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.896\nPrecision  on test set: 0.931\nRecall     on test set: 0.882\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan kernel : linear\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.762\nF1_score   on test set: 0.757\nPrecision  on test set: 0.755\nRecall     on test set: 0.762\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.738\nF1_score   on test set: 0.734\nPrecision  on test set: 0.734\nRecall     on test set: 0.742\n\n-----------------------------------------------------------------------------------------------------------------------------\nMenggunakan kernel : rbf\nMenggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.929\nF1_score   on test set: 0.923\nPrecision  on test set: 0.946\nRecall     on test set: 0.912\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.896\nPrecision  on test set: 0.931\nRecall     on test set: 0.882\n\n-----------------------------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "kernels=['poly','linear','rbf']\n",
    "for i, value in enumerate(kernels):\n",
    "    print(\"Menggunakan kernel : \"+str(value))\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Menggunakan Standard Scaler\")\n",
    "    svl=SVC(kernel= value, C=1000,gamma='scale',tol=0.0001,class_weight='balanced',degree=3,coef0=1)\n",
    "    svl.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(svl, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= svl.predict(feature_test)\n",
    "    scores(svl, prediction, feature_test, label_test)\n",
    "    print()\n",
    "\n",
    "    feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "    print(\"Menggunakan Power Transform Scaler\")\n",
    "    svl=SVC(kernel= value, C=1000,gamma='scale',tol=0.0001,class_weight='balanced',degree=3,coef0=1)\n",
    "    svl.fit(feature_train,label_train)\n",
    "    cross_val_= cross_val_score(svl, feature_train,label_train, cv=5)\n",
    "    print(\"Cross Validation Score : \"+str(cross_val))\n",
    "    prediction= svl.predict(feature_test)\n",
    "    scores(svl, prediction, feature_test, label_test)\n",
    "    print()\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Menggunakan Standard Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.929\nF1_score   on test set: 0.802\nPrecision  on test set: 0.802\nRecall     on test set: 0.802\n\nMenggunakan Power Transform Scaler\nCross Validation Score : [0.70588235 0.57575758 0.75757576 0.78787879 0.78787879]\nAccuracy   on test set: 0.905\nF1_score   on test set: 0.757\nPrecision  on test set: 0.755\nRecall     on test set: 0.762\n\n"
    }
   ],
   "source": [
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_1, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Standard Scaler\")\n",
    "linearsvc = LinearSVC(class_weight='balanced',max_iter=1000)\n",
    "linearsvc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(linearsvc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= linearsvc.predict(feature_test)\n",
    "scores(svl, prediction, feature_test, label_test)\n",
    "print()\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(data_x_2, label, test_size = 0.20, random_state=4)\n",
    "print(\"Menggunakan Power Transform Scaler\")\n",
    "linearsvc = LinearSVC(class_weight='balanced',max_iter=1000)\n",
    "linearsvc.fit(feature_train,label_train)\n",
    "cross_val_= cross_val_score(linearsvc, feature_train,label_train, cv=5)\n",
    "print(\"Cross Validation Score : \"+str(cross_val))\n",
    "prediction= linearsvc.predict(feature_test)\n",
    "scores(svl, prediction, feature_test, label_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kesimpulan\n",
    "\n",
    "Pada percobaan diatas, menggunakan dataset 'Diabetes'. digunakan 10 algoritma untuk mengklasifikasi data menjadi 2 kelas\n",
    "yaitu 0 = 'Tidak terkena Diabetes' , 1 = 'Terkena Diabetes' .\n",
    "\n",
    "Dataset tersebut di-scaling dengan 2 scaler, yaitu 'Standard scaler' dan 'Power Transform'.\n",
    "\n",
    "* Algoritma LogisticRegression mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.810 dan 0.802 , Power Transform : 0.810 dan 0,802\n",
    "\n",
    "* Algoritma MLP Classifier mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.905 dan 0.931 , Power Transform : 0.905 dan 0.911\n",
    "\n",
    "* Algoritma RandomForest Classifier mendapat nilai akurasi dan precision\n",
    "yang terbaik ada pada nilai estimator 100, dengan :\n",
    "\n",
    "    Standard scaler : 0.881 dan 0.893 , Power Transform : 0.881 dan 0.893\n",
    "\n",
    "* Algoritma KNN Classifier mendapat nilai akurasi dan precision\n",
    "yang terbaik ada pada nilai k = 99, dengan :\n",
    "\n",
    "    Standard scaler : 0.857 dan 0.875 , Power Transform : 0.857 dan 0.875\n",
    "\n",
    "* Algoritma Adaboost Classifier mendapat nilai akurasi dan precision\n",
    "yang terbaik ada pada nilai estimator 5, dengan :\n",
    "\n",
    "    Standard scaler : 0.762 dan 0.808 , Power Transform : 0.762 dan 0.808\n",
    "\n",
    "* Algoritma Decision Tree Classifier mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.714 dan 0.704 , Power Transform : 0.762 dan 0.756\n",
    "\n",
    "* Algoritma Gaussian Naive-Bayes mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.619 dan 0.659 , Power Transform : 0.643 dan 0.632\n",
    "\n",
    "* Algoritma SVC yang mendapat nilai terbaik menggunakan kernel 'rbf'\n",
    "dengan nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.929 dan 0.946 , Power Transform : 0.905 dan 0.931\n",
    "\n",
    "* Algoritma LinearSVC mendapat nilai akurasi dan precision\n",
    "\n",
    "    Standard scaler : 0.929 dan 0.802 , Power Transform : 0.905, 0.755\n",
    "\n",
    "Dari hasil percobaan diatas, algoritma SVC-rbf dan LinearSVC, menggunakan data yang\n",
    "di-scaling dengan Standard scaler mendapat hasil akurasi yang terbaik sebesar 0.929\n",
    "\n",
    "Maka, dapat disimpulkan bahwa penggunaan Standard scaler pada dataset ini sudah mencukupi\n",
    "untuk bisa mendapat nilai akurasi dan precision yang cukup baik pada kebanyakan algoritma yang digunakan. \n",
    "Tapi, dalam beberapa algoritma akan lebih baik hasilnya jika menggunakan Power Transform scaler.\n",
    "\n",
    "Hasil klasifikasi yang didapatkan oleh model algoritma dapat dipengaruhi oleh banyak faktor,\n",
    "salah satunya adalah penggunaan Hyperparameter pada tiap algoritma dan keseimbangan jumlah sebaran data pada label.\n",
    "jika seimbang, maka pengklasifikasian data akan terjadi merata / tidak condong pada 1 kelas."
   ]
  }
 ]
}